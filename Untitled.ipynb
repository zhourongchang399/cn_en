{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6466 1813 6467 ...    0    0    0]\n",
      " [6466 1813 6467 ...    0    0    0]\n",
      " [6466 6065 6467 ...    0    0    0]\n",
      " ...\n",
      " [6466 1014  112 ...    0    0    0]\n",
      " [6466   61 6249 ...    0    0    0]\n",
      " [   4  780   41 ...   20  917 6467]]\n",
      "tf.Tensor(\n",
      "[6466   18 3383 6467    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[5339  420  844   11 5340    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file = open(\"./data/cmn.txt\",'r', encoding='UTF-8')\n",
    "all_lines = file.readlines()\n",
    "enlish = []\n",
    "chines = []\n",
    "for line in all_lines:\n",
    "    enlish.append(line.strip().split()[:-11])\n",
    "    chines.append(line.strip().split()[-11])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "enl = []\n",
    "for i in enlish:\n",
    "    enl.append(' '.join(i).replace('.','').replace('!','').replace('?',''))\n",
    "chi = []\n",
    "for i in chines:\n",
    "    chi.append(''.join(i).replace('。','').replace('！','').replace('？',''))\n",
    "\n",
    "\n",
    "\n",
    "enlish =  enl\n",
    "chines = chi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(( e for e in enlish), target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "#tokenizer_cn = tfds.features.text.SubwordTextEncoder.build_from_corpus((' '.join(e) for e in chines), target_vocab_size=2**13)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file('./en_word')\n",
    "tokenizer_cn = tfds.features.text.SubwordTextEncoder.load_from_file('./cn_word')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cn = [tokenizer_cn.encode(line)for line in chines]\n",
    "\n",
    "\n",
    "\n",
    "en= [tokenizer_en.encode(line)for line in enlish]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "en_ = []\n",
    "for i in en :\n",
    "    en_.append([tokenizer_en.vocab_size]+list(i)+[tokenizer_en.vocab_size+1])\n",
    "cn_ = []\n",
    "for i in cn :\n",
    "    cn_.append([tokenizer_cn.vocab_size]+list(i)+[tokenizer_cn.vocab_size+1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "en_text=tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    en_, maxlen=40, dtype='int32', padding='post',\n",
    "    value=0.0)\n",
    "cn_text=tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    cn_, maxlen=40, dtype='int32', padding='post',\n",
    "    value=0.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(en_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset= tf.data.Dataset.from_tensor_slices((en_text,cn_text))\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=200).batch(64)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "de_batch, en_batch = next(iter(train_dataset))\n",
    "\n",
    "print(de_batch[56])\n",
    "print(en_batch[56])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6466 1813 6467 ...    0    0    0]\n",
      " [6466 1813 6467 ...    0    0    0]\n",
      " [6466 6065 6467 ...    0    0    0]\n",
      " ...\n",
      " [6466 1014  112 ...    0    0    0]\n",
      " [6466   61 6249 ...    0    0    0]\n",
      " [   4  780   41 ...   20  917 6467]]\n",
      "tf.Tensor(\n",
      "[6466    1  238 6467    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[5339  147 1962   11 5340    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0], shape=(40,), dtype=int32)\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import en_cn\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    # 这里的i等价与上面公式中的2i和2i+1\n",
    "    angle_rates = 1 / np.power(10000, (2*(i // 2))/ np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis,:],\n",
    "                           d_model)\n",
    "    # 第2i项使用sin\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    # 第2i+1项使用cos\n",
    "    cones = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cones], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "def create_padding_mark(seq):\n",
    "    # 获取为0的padding项\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "    # 扩充维度以便用于attention矩阵\n",
    "    return seq[:, np.newaxis, np.newaxis, :] # (batch_size,1,1,seq_len)\n",
    "\n",
    "# mark 测试\n",
    "\n",
    "\n",
    "\n",
    "# look-ahead mask 用于对未预测的token进行掩码\n",
    "# 这意味着要预测第三个单词，只会使用第一个和第二个单词。 要预测第四个单词，仅使用第一个，第二个和第三个单词，依此类推。\n",
    "\n",
    "def create_look_ahead_mark(size):\n",
    "    # 1 - 对角线和取下三角的全部对角线（-1->全部）\n",
    "    # 这样就可以构造出每个时刻未预测token的掩码\n",
    "    mark = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mark  # (seq_len, seq_len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # query key 相乘获取匹配关系\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    # 使用dk进行缩放\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # 掩码\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # 通过softmax获取attention权重\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    # attention 乘上value\n",
    "    output = tf.matmul(attention_weights, v) # （.., seq_len_v, depth）\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 构造mutil head attention层\n",
    "class MutilHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MutilHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # d_model 必须可以正确分为各个头\n",
    "        assert d_model % num_heads == 0\n",
    "        # 分头后的维度\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # 分头, 将头个数的维度 放到 seq_len 前面\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # 分头前的前向网络，获取q、k、v语义\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        # 分头\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        # 通过缩放点积注意力层\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "        # 把多头维度后移\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3]) # (batch_size, seq_len_v, num_heads, depth)\n",
    "\n",
    "        # 合并多头\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 全连接重塑\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, diff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(diff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        self.eps = epsilon\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=tf.ones_initializer(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=tf.zeros_initializer(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ddf, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MutilHeadAttention(d_model, n_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, ddf)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training, mask):\n",
    "        # 多头注意力网络\n",
    "        att_output, _ = self.mha(inputs, inputs, inputs, mask)\n",
    "        att_output = self.dropout1(att_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + att_output)  # (batch_size, input_seq_len, d_model)\n",
    "        # 前向网络\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        return out2\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, drop_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MutilHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MutilHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(drop_rate)\n",
    "        self.dropout2 = layers.Dropout(drop_rate)\n",
    "        self.dropout3 = layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self,inputs, encode_out, training,\n",
    "             look_ahead_mask, padding_mask):\n",
    "        # masked muti-head attention\n",
    "        att1, att_weight1 = self.mha1(inputs, inputs, inputs,look_ahead_mask)\n",
    "        att1 = self.dropout1(att1, training=training)\n",
    "        out1 = self.layernorm1(inputs + att1)\n",
    "        # muti-head attention\n",
    "        att2, att_weight2 = self.mha2(encode_out, encode_out, inputs, padding_mask)\n",
    "        att2 = self.dropout2(att2, training=training)\n",
    "        out2 = self.layernorm2(out1 + att2)\n",
    "\n",
    "        ffn_out = self.ffn(out2)\n",
    "        ffn_out = self.dropout3(ffn_out, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_out)\n",
    "\n",
    "        return out3, att_weight1, att_weight2\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                input_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "        self.encode_layer = [EncoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                            for _ in range(n_layers)]\n",
    "\n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "    def call(self, inputs, training, mark):\n",
    "\n",
    "        seq_len = inputs.shape[1]\n",
    "        word_emb = self.embedding(inputs)\n",
    "        word_emb *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        emb = word_emb + self.pos_embedding[:,:seq_len,:]\n",
    "        x = self.dropout(emb, training=training)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.encode_layer[i](x, training, mark)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                target_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "        self.decoder_layers= [DecoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                             for _ in range(n_layers)]\n",
    "\n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_out,training,\n",
    "             look_ahead_mark, padding_mark):\n",
    "\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        attention_weights = {}\n",
    "        h = self.embedding(inputs)\n",
    "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        h += self.pos_embedding[:,:seq_len,:]\n",
    "\n",
    "        h = self.dropout(h, training=training)\n",
    "#         print('--------------------\\n',h, h.shape)\n",
    "        # 叠加解码层\n",
    "        for i in range(self.n_layers):\n",
    "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
    "                                                   training, look_ahead_mark,\n",
    "                                                   padding_mark)\n",
    "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
    "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
    "\n",
    "        return h, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, diff,\n",
    "                input_vocab_size, target_vocab_size,\n",
    "                max_seq_len, drop_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads,diff,\n",
    "                              input_vocab_size, max_seq_len, drop_rate)\n",
    "\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, diff,\n",
    "                              target_vocab_size, max_seq_len, drop_rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    def call(self, inputs, targets, training, encode_padding_mask,\n",
    "            look_ahead_mask, decode_padding_mask):\n",
    "\n",
    "        encode_out = self.encoder(inputs, training, encode_padding_mask)\n",
    "        print(encode_out.shape)\n",
    "        decode_out, att_weights = self.decoder(targets, encode_out, training,\n",
    "                                               look_ahead_mask, decode_padding_mask)\n",
    "        print(decode_out.shape)\n",
    "        final_out = self.final_layer(decode_out)\n",
    "\n",
    "        return final_out, att_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_fun(y_ture, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1\n",
    "    loss_ = loss_object(y_ture, y_pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# 构建掩码\n",
    "def create_mask(inputs,targets):\n",
    "    encode_padding_mask = create_padding_mark(inputs)\n",
    "    # 这个掩码用于掩输入解码层第二层的编码层输出\n",
    "    decode_padding_mask = create_padding_mark(inputs)\n",
    "\n",
    "    # look_ahead 掩码， 掩掉未预测的词\n",
    "    look_ahead_mask = create_look_ahead_mark(tf.shape(targets)[1])\n",
    "    # 解码层第一层得到padding掩码\n",
    "    decode_targets_padding_mask = create_padding_mark(targets)\n",
    "\n",
    "    # 合并解码层第一层掩码\n",
    "    combine_mask = tf.maximum(decode_targets_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return encode_padding_mask, combine_mask, decode_padding_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    tar_inp = targets[:,:-1]\n",
    "    tar_real = targets[:,1:]\n",
    "    # 构造掩码\n",
    "    encode_padding_mask, combined_mask, decode_padding_mask = create_mask(inputs, tar_inp)\n",
    "\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inputs, tar_inp,\n",
    "                                    True,\n",
    "                                    encode_padding_mask,\n",
    "                                    combined_mask,\n",
    "                                    decode_padding_mask)\n",
    "        loss = loss_fun(tar_real, predictions)\n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    # 反向传播\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    # 记录loss和准确率\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "epoch 1, batch 0, loss:0.2940, acc:0.0549\n",
      "epoch 1, batch 50, loss:0.2255, acc:0.1273\n",
      "epoch 1, batch 100, loss:0.2365, acc:0.1484\n",
      "epoch 1, batch 150, loss:0.2567, acc:0.1626\n",
      "epoch 1, batch 200, loss:0.2800, acc:0.1729\n",
      "epoch 1, batch 250, loss:0.2999, acc:0.1813\n",
      "epoch 1, batch 300, loss:0.3294, acc:0.1886\n",
      "epoch 1, batch 350, loss:0.3773, acc:0.1967\n",
      "(20, 40, 128)\n",
      "(20, 39, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|▏                                                                             | 1/500 [08:22<69:40:09, 502.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss:0.4105, acc:0.2011\n",
      "time in 1 epoch:502.624933719635 secs\n",
      "\n",
      "epoch 2, batch 0, loss:0.2713, acc:0.0561\n",
      "epoch 2, batch 50, loss:0.2187, acc:0.1275\n",
      "epoch 2, batch 100, loss:0.2294, acc:0.1493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "max_seq_len = 40\n",
    "dropout_rate = 0.1\n",
    "checkpoint_path = './checkpoint/train2'\n",
    "EPOCHS = 500\n",
    "    #checkpoint_path = './train_model'\n",
    "train_dataset = en_cn.train_dataset\n",
    "tokenizer_en = en_cn.tokenizer_en\n",
    "tokenizer_cn = en_cn.tokenizer_cn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_vocab_size = tokenizer_en.vocab_size + 2\n",
    "target_vocab_size = tokenizer_cn.vocab_size + 2\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                              input_vocab_size, target_vocab_size,\n",
    "                              max_seq_len, dropout_rate)\n",
    "learing_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9,beta_2=0.98, epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                               optimizer=optimizer)\n",
    "    # ckpt管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "#checkpoint = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "ckpt.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    start = time.time()\n",
    "        # 重置记录项\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "    for batch, (inputs,targets) in enumerate(train_dataset):\n",
    "\n",
    "            # 训练\n",
    "        train_step(inputs, targets)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print('epoch {}, batch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "                    epoch + 1, batch, train_loss.result(), train_accuracy.result()\n",
    "                ))\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('epoch {}, save model at {}'.format(\n",
    "                epoch + 1, ckpt_save_path\n",
    "            ))\n",
    "\n",
    "    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "            epoch + 1, train_loss.result(), train_accuracy.result()\n",
    "        ))\n",
    "\n",
    "    print('time in 1 epoch:{} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "tensrorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
